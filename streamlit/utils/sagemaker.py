from typing import Dict, List
from langchain.embeddings import SagemakerEndpointEmbeddings
from langchain import PromptTemplate, SagemakerEndpoint
from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler
from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.llms.sagemaker_endpoint import LLMContentHandler
from langchain.embeddings.base import Embeddings
from langchain.llms.utils import enforce_stop_tokens
from pydantic import Extra, BaseModel
from langchain.llms.base import LLM
import json
import os
import boto3
from botocore.config import Config
import logging
import requests
from typing import Optional, Any, Mapping


logger = logging.getLogger('retail_genai')
logger.setLevel(logging.DEBUG)
logger.addHandler(logging.StreamHandler())

TEXT_EMBEDDING_MODEL_ENDPOINT_NAME = 'huggingface-textembedding-gpt-j-6b-fp16-1691075868'
TEXT_MODEL_ENDPOINT_NAME = 'huggingface-text2text-flan-t5-xxl-1692735308'

MAX_LENGTH = 256
NUM_RETURN_SEQUENCES = 1
TOP_K = 0
TOP_P = 0.7
DO_SAMPLE = True 

key = os.environ.get('AWS_ACCESS_KEY_ID', '')
secret = os.environ.get('AWS_SECRET_ACCESS_KEY','')
region = os.environ.get('AWS_DEFAULT_REGION','eu-west-1')
credentials_relative_uri = os.environ.get('AWS_CONTAINER_CREDENTIALS_RELATIVE_URI','')
sessionToken = None

# logger.info(f'key: {key}')
# logger.info(f'secret: {secret}')
# logger.info(f'region: {region}')

credentials_url = f'http://169.254.170.2{credentials_relative_uri}'
if not key or not secret:
    response = requests.get(credentials_url)
    response_json = response.json()
    #logger.info(f'response_json: {response_json}')
    key = response_json['AccessKeyId']
    secret = response_json['SecretAccessKey']
    sessionToken = response_json['Token']
    # logger.info(f'new key: {key}')
    # logger.info(f'new secret: {secret}')

class CustomSageMakerLLM(LLM):
    client: Any
    endpoint_name: str = ""
    region_name: str = ""
    credentials_profile_name: Optional[str] = None
    content_handler: LLMContentHandler
    model_kwargs: Optional[Dict] = None
    endpoint_kwargs: Optional[Dict] = None

    class Config:
        """Configuration for this pydantic object."""

        extra = Extra.forbid

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """Get the identifying parameters."""
        _model_kwargs = self.model_kwargs or {}
        return {
            **{"endpoint_name": self.endpoint_name},
            **{"model_kwargs": _model_kwargs},
        }

    @property
    def _llm_type(self) -> str:
        """Return type of llm."""
        return "sagemaker_endpoint"

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """Call out to Sagemaker inference endpoint.

        Args:
            prompt: The prompt to pass into the model.
            stop: Optional list of stop words to use when generating.

        Returns:
            The string generated by the model.

        Example:
            .. code-block:: python

                response = se("Tell me a joke.")
        """
        _model_kwargs = self.model_kwargs or {}
        _model_kwargs = {**_model_kwargs, **kwargs}
        _endpoint_kwargs = self.endpoint_kwargs or {}

        body = self.content_handler.transform_input(prompt, _model_kwargs)
        content_type = self.content_handler.content_type
        accepts = self.content_handler.accepts

        # send request
        try:
            response = self.client.invoke_endpoint(
                EndpointName=self.endpoint_name,
                Body=body,
                ContentType=content_type,
                Accept=accepts,
                **_endpoint_kwargs,
            )
        except Exception as e:
            raise ValueError(f"Error raised by inference endpoint: {e}")

        text = self.content_handler.transform_output(response["Body"])
        if stop is not None:
            # This is a bit hacky, but I can't figure out a better way to enforce
            # stop tokens when making calls to the sagemaker endpoint.
            text = enforce_stop_tokens(text, stop)

        return text    

class CustomSageMakerEmbeddings(BaseModel, Embeddings):
    client: Any
    endpoint_name: str = ""
    region_name: str = ""
    credentials_profile_name: Optional[str] = None
    content_handler: EmbeddingsContentHandler
    model_kwargs: Optional[Dict] = None
    endpoint_kwargs: Optional[Dict] = None

    class Config:
        """Configuration for this pydantic object."""

        extra = Extra.forbid
        arbitrary_types_allowed = True

    def _embedding_func(self, texts: List[str]) -> List[List[float]]:
        """Call out to SageMaker Inference embedding endpoint."""
        # replace newlines, which can negatively affect performance.
        texts = list(map(lambda x: x.replace("\n", " "), texts))
        _model_kwargs = self.model_kwargs or {}
        _endpoint_kwargs = self.endpoint_kwargs or {}

        body = self.content_handler.transform_input(texts, _model_kwargs)
        content_type = self.content_handler.content_type
        accepts = self.content_handler.accepts

        # send request
        try:
            response = self.client.invoke_endpoint(
                EndpointName=self.endpoint_name,
                Body=body,
                ContentType=content_type,
                Accept=accepts,
                **_endpoint_kwargs,
            )
        except Exception as e:
            raise ValueError(f"Error raised by inference endpoint: {e}")

        return self.content_handler.transform_output(response["Body"])
    
    def embed_documents(
        self, texts: List[str], chunk_size: int = 64
    ) -> List[List[float]]:
        """Compute doc embeddings using a SageMaker Inference Endpoint.

        Args:
            texts: The list of texts to embed.
            chunk_size: The chunk size defines how many input texts will
                be grouped together as request. If None, will use the
                chunk size specified by the class.


        Returns:
            List of embeddings, one for each text.
        """
        results = []
        _chunk_size = len(texts) if chunk_size > len(texts) else chunk_size
        for i in range(0, len(texts), _chunk_size):
            response = self._embedding_func(texts[i : i + _chunk_size])
            results.extend(response)
        return results
    
    def embed_query(self, text: str) -> List[float]:
        """Compute query embeddings using a Bedrock model.

        Args:
            text: The text to embed.

        Returns:
            Embeddings for the text.
        """
        return self._embedding_func(text)
    

class ContentHandler_Embeddings(EmbeddingsContentHandler):
    content_type = "application/json"
    accepts = "application/json"

    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:
        input_str = json.dumps({"inputs": inputs, **model_kwargs})
        return input_str.encode("utf-8")

    def transform_output(self, output: bytes) -> List[List[float]]:
        response_json = json.loads(output.read().decode("utf-8"))
        return response_json["vectors"]

class ContentHandler_LLM(LLMContentHandler):
    content_type = "application/json"
    accepts = "application/json"

    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:
        input_str = json.dumps({'text_inputs': prompt, **model_kwargs})
        return input_str.encode("utf-8")

    def transform_output(self, output: bytes) -> str:
        response_json = json.loads(output.read().decode("utf-8"))
        output = response_json['generated_texts'][0]
        logger.info(f'output: {output}')
        return output
        


def get_sagemaker_client(
    assumed_role: Optional[str] = None,
    aws_access_key_id: Optional[str] = key,
    aws_secret_access_key: Optional[str] = secret,
    aws_session_token: Optional[str] = sessionToken,
):
    """Create a boto3 client for Amazon Sagemaker, with optional configuration overrides

    Parameters
    ----------
    assumed_role :
        Optional ARN of an AWS IAM role to assume for calling the Bedrock service. If not
        specified, the current active credentials will be used.
    endpoint_url :
        Optional override for the Bedrock service API Endpoint. If setting this, it should usually
        include the protocol i.e. "https://..."
    region :
        Optional name of the AWS Region in which the service should be called (e.g. "us-east-1").
        If not specified, AWS_REGION or AWS_DEFAULT_REGION environment variable will be used.
    """
    if region is None:
        target_region = os.environ.get("AWS_REGION", os.environ.get("AWS_DEFAULT_REGION"))
    else:
        target_region = region

    print(f"Create new client\n  Using region: {target_region}")
    session_kwargs = {"region_name": target_region, "aws_access_key_id" : aws_access_key_id, "aws_secret_access_key" : aws_secret_access_key, "aws_session_token": aws_session_token}
    client_kwargs = {**session_kwargs}

    profile_name = os.environ.get("AWS_PROFILE")
    if profile_name:
        print(f"  Using profile: {profile_name}")
        session_kwargs["profile_name"] = profile_name

    retry_config = Config(
        region_name=target_region,
        retries={
            "max_attempts": 5,
            "mode": "standard",
        },
    )
    session = boto3.Session(**session_kwargs)

    if assumed_role:
        print(f"  Using role: {assumed_role}", end='')
        sts = session.client("sts")
        response = sts.assume_role(
            RoleArn=str(assumed_role),
            RoleSessionName="langchain-llm-1"
        )
        print(" ... successful!")
        client_kwargs["aws_access_key_id"] = response["Credentials"]["AccessKeyId"]
        client_kwargs["aws_secret_access_key"] = response["Credentials"]["SecretAccessKey"]
        client_kwargs["aws_session_token"] = response["Credentials"]["SessionToken"]


    sagemaker_client = session.client(
        service_name="sagemaker-runtime",
        config=retry_config,
        **client_kwargs
    )

    print("boto3 sagemaker client successfully created!")
    print(sagemaker_client._endpoint)
    return sagemaker_client


def get_sm_embeddings(endpointName = TEXT_EMBEDDING_MODEL_ENDPOINT_NAME):
    content_handler = ContentHandler_Embeddings()
    embeddings = CustomSageMakerEmbeddings(
        client= get_sagemaker_client(),
        endpoint_name= endpointName,
        content_handler=content_handler,
    )
    return embeddings

def get_sm_llm(endpointName = TEXT_MODEL_ENDPOINT_NAME, model_kwargs  = {}):
    content_handler = ContentHandler_LLM()
    embeddings = CustomSageMakerLLM(
        client= get_sagemaker_client(),
        endpoint_name= endpointName,
        model_kwargs  = model_kwargs ,
        content_handler=content_handler,
    )
    return embeddings


def get_text_sm(prompt, endpointName = TEXT_MODEL_ENDPOINT_NAME, generationConfig = None):

    if prompt is None or prompt == '':
        return
    
    boto3_sm = get_sagemaker_client(
    aws_access_key_id= key,
    aws_secret_access_key = secret,
    aws_session_token= sessionToken
    )

    accept = 'application/json'
    contentType = 'application/json'

    payload = {'text_inputs': prompt, 
    'max_length': MAX_LENGTH, 
    'num_return_sequences': NUM_RETURN_SEQUENCES,
    'top_k': TOP_K,
    'top_p': TOP_P,
    'do_sample': DO_SAMPLE}
    
    if generationConfig is None:
        payload = {'text_inputs': prompt, 
           'max_length': MAX_LENGTH, 
           'num_return_sequences': NUM_RETURN_SEQUENCES,
           'top_k': TOP_K,
           'top_p': TOP_P,
           'do_sample': DO_SAMPLE}
    
    body = json.dumps(payload).encode('utf-8')


    response = boto3_sm.invoke_endpoint(EndpointName=endpointName, 
                                  ContentType=contentType, 
                                  Accept="application/json",
                                  Body=body)
    response_body = json.loads(response['Body'].read())

    logger.info(response_body)

    outputText = response_body['generated_texts'][0]
    return outputText

